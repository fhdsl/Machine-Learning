[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "1 Machine Learning",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Machine Learning",
    "section": "1.1 Target Audience",
    "text": "1.1 Target Audience\nThe course is intended for folks with basic Python programming experience who are interested in predictive modeling and model interpretation from a machine learning perspective. The course is also appropriate for scientists and clinicians who are interested to communicate with data scientists to understand the ins and outs of a machine learning problem. The pre-requisites for the course is Intro to Python, or being able to use Lists and Pandas Dataframes to manipulate data. Basic knowledge of statistics, such as hypothesis testing and p-values, is also strongly recommended.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#curriculum",
    "href": "index.html#curriculum",
    "title": "Machine Learning",
    "section": "1.2 Curriculum",
    "text": "1.2 Curriculum\nThe course covers the framework of machine learning for predictive modeling and model interpretation from a practitioner’s perspective. You will be able to implement several popular machine learning techniques based on the question of interest and the dataset at hand. You will then evaluate the model based on their performance and diagnostics to understand its strengths and limitations. Technical mathematics and algorithms will not be emphasized.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Machine Learning",
    "section": "1.3 Learning Objectives",
    "text": "1.3 Learning Objectives\n\nCompare machine learning models in terms of flexibility vs. Interpretability.\nCompare machine learning model performance in terms of overfitting and underfitting.\nImplement and Interpret models such as linear regression, logistic regression, and lasso using a Tidy dataset via existing packages such as Sklearn and Statsmodels.\nEvaluate model performance metrics for inference and prediction, such as AIC, BIC, MSE, and AUC, under a cross validated framework if appropriate.\nExplain the difference in machine learning techniques between low and high dimensional data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html",
    "href": "01-Problem-Setup.html",
    "title": "2  The Whole Game",
    "section": "",
    "text": "2.1 Learning Objectives for today",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#learning-objectives-for-today",
    "href": "01-Problem-Setup.html#learning-objectives-for-today",
    "title": "2  The Whole Game",
    "section": "",
    "text": "Describe the main use cases of a Machine Learning model.\nDescribe the overarching workflow of Machine Learning:\n\nDescribe the importance of allocating the data into Training and Testing sets.\nDescribe some criteria for choosing predictors for a Machine Learning model.\nDescribe the process of evaluating a Machine Learning model.\n\nDescribe the components of a Python Data Structure.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#an-conceptual-example",
    "href": "01-Problem-Setup.html#an-conceptual-example",
    "title": "2  The Whole Game",
    "section": "2.2 An conceptual example",
    "text": "2.2 An conceptual example\nSuppose that we are given the National Health And Nutrition Examination Survey (NHANES) dataset and want to build a machine learning model to predict a person’s Mean Blood Pressure (also known as Mean Arterial Pressure), which is related to person’s diastolic and systolic blood pressures.\nThe Mean Blood Pressure is our Response Variable or Outcome we wish to make predictions on, given Predictor Variables we have in the NHANES dataset. We have ~70 predictors we can consider, some of which include: Age, BMI, Average Number of Hours Slept, Gender, Income, Highest Education, Martial Status, Mental health questionnaires, and so forth.\nSuppose that we decide to use the predictors \\(Age\\) and \\(BMI\\) for our machine learning model, and formulate it algebraically:\n\\[\nMeanBloodPressure=f(Age, BMI)\n\\]\nwhere \\(f(Age, BMI)\\) is a function that represents a machine learning model that takes in the predictors \\(Age\\), \\(BMI\\), and make a prediction on one’s \\(MeanBloodPressure\\). We use some of the data to build this machine learning model:\n\\[\nMeanBloodPressure= 20 + 3 \\cdot Age - .2 \\cdot BMI\n\\]\nHow did we arrive at this equation form and the numbers \\(20\\), \\(3\\), and \\(-.2\\)? They were estimated from the data as well as some assumptions of the equation form to be Linear.\nGiven this model, suppose are given a person’s \\(Age\\) to be 30 and \\(BMI\\) to be 34. Then, we can make a prediction using this model:\n\\[\nMeanBloodPressure=20 + 3 \\cdot 30 - .2 \\cdot 34 = 103.2\n\\]\nTo see how accurate our prediction is, we can compare our predicted response to the true response, if we have the data for that. That will give us feedback on how well our model is and let us make improvements to it.\n\n2.2.1 Broader Usage\nA machine learning model, such as the one described above, has two main uses:\n\nClassification and Prediction (Focus of this course): How accurately can we predict or classify the outcome?\n\nPrediction: Given a person’s \\(Age, BMI\\), predict the person’s \\(MeanBloodPressure\\) value. The outcome is a continuous value.\nClassification: This is when the response is a categorical value, such as Yes/No. As an example, we can consider classifying Yes/No whether a person has \\(Hypertension\\) or not using predictors \\(Age\\) and \\(BMI\\).\n\nInference (Secondary in this course): Which predictors are associated with the response, and how strong is the association?\n\nPrediction model example: Using the linear model above as an example, each predictor has a relationship to the outcome: an increase of \\(Age\\) by 1 will lead to an increase of \\(MeanBloodPressure\\) by 3. This measures the strength of association between a variable and the outcome.\nClassification model example: What is the odds ratio of of \\(Age\\) on \\(Hyptertension\\)? If the odds ratio of \\(Age\\) on \\(Hyptertension\\) is 2, then an increase of 1 in \\(Age\\) increases the odds of \\(Hyptertension\\) by 2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#the-conceptual-example-in-more-depth",
    "href": "01-Problem-Setup.html#the-conceptual-example-in-more-depth",
    "title": "2  The Whole Game",
    "section": "2.3 The conceptual example, in more depth",
    "text": "2.3 The conceptual example, in more depth\nLet’s walk through our conceptual example again, but in more depth, and with code examples. This will illustrate the complexity and the strategies involved in the Machine Learning roadmap we will explore carefully throughout the course.\n\n2.3.1 Visualizing the outcome\nBuilding a sound machine learning model requires careful understanding of the data, and we often start looking at the response variable.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom formulaic import model_matrix\nimport statsmodels.api as sm\n\nnhanes = pd.read_csv(\"classroom_data/NHANES.csv\")\nnhanes.drop_duplicates(inplace=True)\nnhanes['MeanBloodPressure'] = nhanes['BPDiaAve'] + (nhanes['BPSysAve'] - nhanes['BPDiaAve']) / 3 \n\nplt.clf()\ng = sns.displot(x=\"MeanBloodPressure\", data=nhanes)\ng.refline(x=70, color='r', linestyle='--')\ng.refline(x=90, color='r', linestyle='--')\nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe see that the \\(MeanBloodPressure\\) is fairly symmetrically distributed. A person has normal blood pressure if their \\(MeanBloodPressure\\) is between values of 70 and ~90, which we have shown in the dotted vertical lines. There seems to be more people with elevated blood pressure, as there are more counts above 80 than below 80.\n\n\n\nMean Arterial Pressure\nInterpretation\n\n\n\n\nHypotension\n&lt;70\n\n\nNormal\n70-92\n\n\nStage 1 Hypertension\n92-96\n\n\nStage 2 Hypertension\n&gt;96\n\n\n\nUsually, a symmetric, continuous distribution for the response variable is a great way to start the machine learning modeling process, as a lot of models assume a symmetric, continuous distribution for the model to perform well.\nIf the response distribution is strongly skewed, then we may want to perform mathematical transformations fix it.\nIf the response distribution has multiple peaks (multi-modal), we may want to perform mathematical transformations, or consider reformulate the problem as a (multi-class) classification problem, if the interpretation makes sense.\n\n\n2.3.2 Splitting the data\nOur dataset has 7832 data points:\n\nprint(nhanes.shape)\n\n(7832, 77)\n\n\nIn Machine Learning, we need to carefully allocate how we want to use our data. In the machine learning model development process, we reserve some of the data, called the Training Set to allow the model learn from the data, and reserve the rest of the data, called the Testing Set to allow the model to be tested on new, unseen data.\nA logistical and psychological challenge of Machine Learning is to not have the model know anything about the Testing Set as you develop it. This is because as the model learns from any data, it will learn to recognize its patterns, and sometimes it will recognize patterns that are only specific to this data and not reproducible anywhere else. This is called Overfitting. This is why we have a separate, unseen testing set to see if the model will generalize its performance.\nIn the previous section, we looked at the response data of the entire dataset, because if we don’t have a lot of data and if the distribution has multiple peaks, we may consider splitting data more carefully.\nBelow, we split the entire data randomly into Training and Testing sets, giving 80% of the data to Training, and 20% of the data to Testing. There are other ways of splitting data to consider, in scenarios such as:\n\nTime series data\nSpatial data\nThe response data is small has multiple peaks\n\nbut random splitting will suffice for this example.\n\nnhanes_train, nhanes_test = train_test_split(nhanes, test_size=0.2, random_state=42)\n\nAnd let’s look at the number of data points after splitting:\n\nprint(\"Training size:\", nhanes_train.shape)\nprint(\"Testing size:\", nhanes_test.shape)\n\nTraining size: (6265, 77)\nTesting size: (1567, 77)\n\n\n\n\n2.3.3 Exploratory Data Analysis\nNow, using only the Training Set, we try to discern which variables might be good predictors of our response, as well as how they relate - is it linear, nonlinear, or something else? There are many ways to pick predictors for a model, ranging from Exploratory Data Analysis to quantitative methods, and we will be more comprehensive later in this course.\nLet’s look at the relationship between \\(MeanBloodPressure\\) and potential predictor \\(BMI\\). We add a smooth line fit to the scatterplot, because it shows the average trend between these two variables. The black dotted lines are the ranges of healthy mean blood pressure from our response histogram.\n\nplt.clf()\nax = sns.regplot(y=\"MeanBloodPressure\", x=\"BMI\", data=nhanes_train, lowess=True, scatter_kws={'alpha':0.2}, line_kws={'color':\"r\"})\nax.axhline(y=70, color='black', linestyle='--')\nax.axhline(y=90, color='black', linestyle='--')\nax.set_xlim([10, 50])\nplt.show()\n\n\n\n\n\n\n\n\nOkay, great, it looks like when someone’s BMI is higher, then it is more likely they have higher mean blood pressure.\nLet’s look at \\(Age\\):\n\nplt.clf()\nax = sns.regplot(y=\"MeanBloodPressure\", x=\"Age\", data=nhanes_train, lowess=True, scatter_kws={'alpha':0.2}, line_kws={'color':\"r\"})\nax.axhline(y=70, color='black', linestyle='--')\nax.axhline(y=90, color='black', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\nWe see a similar trend. How about \\(Gender\\)?\n\nplt.clf()\nax = sns.boxplot(x=\"Gender\", y=\"MeanBloodPressure\", data=nhanes)\nax.axhline(y=70, color='black', linestyle='--')\nax.axhline(y=90, color='black', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\nMales tend to have a higher \\(MeanBloodPressure\\). Let’s look at one more, \\(DirectChol\\):\n\nplt.clf()\nax = sns.regplot(y=\"MeanBloodPressure\", x=\"DirectChol\", data=nhanes, lowess=True, scatter_kws={'alpha':0.1}, line_kws={'color':\"r\"})\nax.axhline(y=70, color='black', linestyle='--')\nax.axhline(y=90, color='black', linestyle='--')\nplt.show()\n\n\n\n\n\n\n\n\nThis has a more nonlinear trend to it.\nGiven our (very brief) Exploratory Data Analysis, let’s pick a few predictors that seem promising: \\(BMI\\), \\(Age\\), \\(Gender\\).\nAs we look at data with a huge number of predictors, we will have to find ways to automate this process called feature selection.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#picking-a-model-linear-regression",
    "href": "01-Problem-Setup.html#picking-a-model-linear-regression",
    "title": "2  The Whole Game",
    "section": "2.4 Picking a model: Linear Regression",
    "text": "2.4 Picking a model: Linear Regression\nWe will explore a whole range of models in this course to try in our predictive model, but one of the most fundamental model that has been robust and connects to nearly all Machine Learning models is Linear Regression. A common strategy is to start with a simple model such as Linear Regression and build out complexity from it. So we gotta see this in action.\nGiven our decided predictors, the model will look like this:\n\\[\nMeanBloodPressure= \\beta_0 + \\beta_1 \\cdot Age + \\beta_2 \\cdot BMI + \\beta_3 \\cdot Gender\n\\]\nwhere the unknown variables \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\), called parameters or coefficients, will be learned in the model training process.\nWe specify this form:\n\ny_train, X_train = model_matrix(\"MeanBloodPressure ~ Age + BMI + Gender\", nhanes_train)\n\nAnd fit the model, which gives our parameters:\n\nfrom sklearn import linear_model\nlinear_reg = linear_model.LinearRegression()\nlinear_reg = linear_reg.fit(X_train, y_train)\nprint(linear_reg.intercept_)\nprint(linear_reg.coef_)\n\n[62.7177058]\n[[0.         0.23590657 0.36493118 3.3580882 ]]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#picking-a-model-decision-tree",
    "href": "01-Problem-Setup.html#picking-a-model-decision-tree",
    "title": "2  The Whole Game",
    "section": "2.5 Picking a model: Decision Tree",
    "text": "2.5 Picking a model: Decision Tree\nA different model is called a Decision Tree. It is composed of a set of hierarchical if/then statements based on the predictors that ends in a node that dictate what the response prediction should be. Below shows an example Decision Tree with three hierarchical if/then statements:\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\n\ndecision_tree = DecisionTreeRegressor(max_depth=3)\ndecision_tree = decision_tree.fit(X_train, y_train)\n\n\ntree.plot_tree(decision_tree, proportion=True)\nplt.show()\n\n\n\n\n\n\n\n\nHere is how we would interpret the leftmost node:\n“If \\(Age\\) is less than 17.5, and less than 12.5, and \\(BMI\\) is less than 22.65, then predict 64.7 for \\(MeanBloodPressure\\).”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#model-evaluation",
    "href": "01-Problem-Setup.html#model-evaluation",
    "title": "2  The Whole Game",
    "section": "2.6 Model Evaluation",
    "text": "2.6 Model Evaluation\nNow that we have fitted our model on the training set, we can see how it performs on the testing set. In our testing set, we already know all the values of the predictors and outcomes, so to simulate a realistic scenario, we feed the predictor to the model, let it make a prediction, and compare it to the true outcome.\nWhen we compare the predicted outcome vs. true outcome, we need to use a metric to compare the two quantities. Here are two popular metrics:\n\nMean Absolute Error (MAE): the average of the absolute difference between predicted vs. true outcome. This scale of this metric is the same as the outcome’s scale, which is easy to interpret.\nMean Squared Error (MSE): the average of the squared difference between predicted vs. true outcome. The scale of this metric is harder to interpret, but it has very nice mathematical properties for the model fitting that it remains very popular.\n\nWe will consider other model evaluation metrics throughout the course, especially in situation when the dataset isn’t big enough for a training and splitting set.\nLet’s look at our MAE of our Linear Regression model on the test data:\n\nfrom sklearn.metrics import mean_absolute_error\ny_test, X_test = model_matrix(\"MeanBloodPressure ~ Age + BMI + Gender\", nhanes_test)\ny_test_predicted = linear_reg.predict(X_test)\n\ntest_err = round(mean_absolute_error(y_test_predicted, y_test), 2)\ntest_err\n\n8.65\n\n\nOkay, on average our model is off by 8.65 on the scale of \\(MeanBloodPressure\\).\nLet’s visualize this:\n\nplt.clf()\nplt.scatter(y_test_predicted, y_test, alpha=.5)\nplt.axline((70, 70), slope=1, color='r', linestyle='--')\nplt.xlabel('Predicted MeanBloodPressure')\nplt.ylabel('True MeanBloodPressure')\nplt.title('Linear Regression MAE:' + str(test_err))\nplt.show()\n\n\n\n\n\n\n\n\nLet’s do the same for the Regression Tree model:\n\ny_test_predicted = decision_tree.predict(X_test)\n\ntest_err = round(mean_absolute_error(y_test_predicted, y_test), 2)\nplt.clf()\nplt.scatter(y_test_predicted, y_test, alpha=.5)\nplt.axline((70, 70), slope=1, color='r', linestyle='--')\nplt.xlabel('Predicted MeanBloodPressure')\nplt.ylabel('True MeanBloodPressure')\nplt.title('Regression Tree MAE:' + str(test_err))\nplt.show()\n\n\n\n\n\n\n\n\nThe predictions look very discrete, because we only allowed the tree to be split into eight categories.\nIn a full analysis, we would look at the performance of each model and ask what can be improved.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#review-of-data-structures",
    "href": "01-Problem-Setup.html#review-of-data-structures",
    "title": "2  The Whole Game",
    "section": "2.7 Review of Data Structures",
    "text": "2.7 Review of Data Structures\nWe will be seeing a lot of different data structures in this course beyond DataFrames, Series, and Lists. So let’s review how we think about learning new data structures to make our lives easier when we encounter new data structures.\nFor any data structure, we ask the following:\n\nWhat does it contain (in terms of data)?\nWhat can it do (in terms of functions)?\n\nAnd if it “makes sense” to us, then it is well-designed data structure.\nFormally, a data structure in Python (also known as an Object) may contain the following:\n\nValue that holds the essential data for the data structure.\nAttributes that hold subset or additional data for the data structure.\nFunctions called Methods that are for the data structure and have to take in the variable referenced as an input.\n\nLet’s see how this applies to the List:\n\nValue: the contents of the list, such as [2, 3, 4].\nAttributes that store additional values: Not relevant for lists.\nMethods that can be used on the object: my_list.append(x)\n\nHow about Dataframe?\n\nValue: the 2-dimensional spreadsheet of the dataframe.\nAttributes that store additional values: df.shape gives the number of rows and columns. df.my_col_name access the column called “my_col_name”.\nMethods that can be used on the object: df.merge(other_df, on=“column_name”)\n\nFeel free to look at the cheatsheet on data structures from Intro to Python to refresh yourself.\n\n2.7.1 NumPy\nA new Data Structure we will work with in this course is NumPy’s ndarray (“n-dimensional array”) data structure. It is commonly referred as “NumPy Array”. It is very similar to a Dataframe, but has the following characteristics for building machine learning models:\n\nAll elements are homogeneous and numeric.\nThere are no column or row names.\nMathematical operations are optimized to be fast.\n\nSo, let’s see some examples:\n\nValue: the 2-dimensional numerical table. It actually can be any dimension, but we will just work with 1-dimensional (similar to a List) and 2-dimensional.\nAttributes that store additional values:\n\nTwo-dimensional subsetting, similar to lists: data[:5, :3] subsets for for the first 5 rows and first three columns. data[:5, [0, 2, 3]] subsets for the first 5 rows and 1st, 3rd, and 4th columns.\ndata.shape gives the shape of the NumPy Array. data.ndim will tell you the number of dimensions of the NumPy Array.\n\nMethods that can be used on the object:\n\ndata.sum(axis=0) sums over rows, data.sum(axis=1) sums over columns.\n\n\nFor this course, we often load in a dataset in the Pandas Dataframe format, and then once we pick the our outcome and predictors, we will transform the Dataframe into an NumPy Array, such as this line of code we saw earlier: y_train, X_train = model_matrix(\"Hypertension ~ BMI\", nhanes_train).\nWe specify our outcome, predictor, and Dataframe for the model_matrix() function, and the outputs are two NumPy Arrays, one for the outcome, and one for the predictors. Any downstream Machine Learning modeling work off the NumPy Arrays y_train and X_train.\nMore introduction can be found on NumPy’s tutorial guide.\n\n\n2.7.2 What is this data structure?\nIf you are not sure about what your variable’s data structure, use the type() function, such as type(mystery_data) and it will tell you.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#exercises",
    "href": "01-Problem-Setup.html#exercises",
    "title": "2  The Whole Game",
    "section": "2.8 Exercises",
    "text": "2.8 Exercises\nExercises for week 1 can be found here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Whole Game</span>"
    ]
  },
  {
    "objectID": "02-Regression.html",
    "href": "02-Regression.html",
    "title": "3  Regression",
    "section": "",
    "text": "3.1 Linear Regression in depth\nLast week, we were exposed to the Linear Regression model. Today we will take it apart and look at it carefully to see how it works and what are the needed assumptions to use this model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#linear-regression-in-depth",
    "href": "02-Regression.html#linear-regression-in-depth",
    "title": "3  Regression",
    "section": "",
    "text": "3.1.1 One predictor\nSuppose we just use one predictor, \\(Age\\) to predict our outcome \\(MeanBloodPressure\\).\n\\[\nMeanBloodPressure= \\beta_0 + \\beta_1 \\cdot Age\n\\]\nOur model would look like the following like the red line from our Training data:\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom formulaic import model_matrix\nfrom sklearn import linear_model\nimport statsmodels.api as sm\n\n\nnhanes = pd.read_csv(\"classroom_data/NHANES.csv\")\nnhanes.drop_duplicates(inplace=True)\nnhanes['MeanBloodPressure'] = nhanes['BPDiaAve'] + (nhanes['BPSysAve'] - nhanes['BPDiaAve']) / 3 \nnhanes_train, nhanes_test = train_test_split(nhanes, test_size=0.2, random_state=42)\n\ny_train, X_train = model_matrix(\"MeanBloodPressure ~ Age\", nhanes_train)\nlinear_reg = linear_model.LinearRegression()\nlinear_reg = linear_reg.fit(X_train, y_train)\ny_train_predicted = linear_reg.predict(X_train)\n\nplt.clf()\nplt.scatter(X_train.Age, y_train, alpha=.2)\nplt.plot(X_train.Age, y_train_predicted, color=\"red\")\nplt.xlabel('Age')\nplt.ylabel('Mean Blood Pressure')\nplt.show()\n\n\n\n\n\n\n\n\nThis model is formed by making the the line of best fit determined by the minimum of the sum of squared difference between the observed response (in blue points) and predicted response (in red line). Using the termology from last week, we say that we fit a line where the Mean Squared Error (MSE) is minimized.\nAnother illustration:\n\n\n\nImage source: https://kenndanielso.github.io/mlrefined/blog_posts/8_Linear_regression/8_1_Least_squares_regression.html\n\n\nLeft panel: difference between response and predicted response (residual). Right panel: squared difference between predicted response and response.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#assumptions-of-linear-regression",
    "href": "02-Regression.html#assumptions-of-linear-regression",
    "title": "3  Regression",
    "section": "3.2 Assumptions of linear regression",
    "text": "3.2 Assumptions of linear regression\nAny model that one uses has some assumptions about the data that allows the model to make good predictions. Note that there are other types of assumptions if your modeling technique is focused on inference.\nLet’s take a look what are the assumptions needed for a sound model, and what we can do to address it not upheld.\n\n3.2.1 Linearity of responder-predictor relationship\nThe linear regression model assumes that there is a straight line (linear) relationship between the predictors and the response. It doesn’t ask for the straight line relationship to be perfect, but rather on average the cloud of points has a linear shape. If that is not true, then our prediction is going to be less accurate.\nTo check for this relationship, we have to calculate the residual, which is the difference between the response value and the predicted response value (similar to a type of model performance metrics we examined last week). Then, we can make a residual plot of the predicted response vs. residual. Ideally, this residual plot should have no pattern - some residuals above 0, some below 0, but no strong trend.\nIf there’s a trend in the data, that means there are non-linear associations between some of the predictors and the response.\n\nresidual = y_train - y_train_predicted\n\nplt.clf()\nplt.scatter(y_train_predicted, residual, alpha=.2)\nplt.xlabel('Predicted response')\nplt.ylabel('Residual')\nplt.show()\n\n\n\n\n\n\n\n\nWe see there’s a slight curve in our residual plot. We will look at ways to deal with this later in this lecture.\n\n\n3.2.2 No Outliers\nAn outlier is an obseravtion for which the response is far from the value predicted response (y-axis). An observation has high leverage if it has an unusual predictor value (x-axis). Outliers and high leverage observations arise may arise out of incorrect measurements, among many other causes. When these observations cause significant changes to the regression model, they are called influential. They may greatly contribute to the Mean Squared Error, as observations away the majority of the data will have exponentially large residuals.\nSome possible solutions:\n\nWe can eyeball for for potential influential points by exploratory data analysis, and see how the model changes if we remove it. We may troubleshoot with the instruments that generated the data in the first place to diagnosis.\nWe can detect an influential point via computing the studentized residuals or cook’s distance and decide whether it makes sense to remove it.\nWe can use a different linear regression method, called Huber loss regression, that allows more tolerance for outliers.\n\n\n\n3.2.3 Predictors are not colinear\nColinearity is the situation when two or more predictors are linearly related to each other. If we put collinear predictors into our regression model, they start to serve as redundant information to our model and can degrade predictive performance.\nSome possible solutions:\n\nWe can detect collinearity to look at the correlation matrix between predictors. This works well for pairwise correlations.\nWhen there is a collinear relationship between three or more predictors, pairwise methods will fail. We may consider the Variance Inflation Factor to detect them, but doesn’t necessarily recommend which variables to remove.\n\nSuppose that we are consider the predictors of our training set:\n\n#some cleanup\nobj_columns = nhanes_train.select_dtypes(['object']).columns\nnhanes_train[obj_columns] = nhanes_train[obj_columns].apply(lambda x: x.astype('category'))\n\ncat_columns = nhanes_train.select_dtypes(['category']).columns\nnhanes_train[cat_columns] = nhanes_train[cat_columns].apply(lambda x: x.cat.codes)\n\n#create correlation matrix\ncorr_matrix = nhanes_train.corr()\n\nplt.clf()\nsns.heatmap(corr_matrix, cmap='coolwarm')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s look at a pair of predictors up close:\n\nplt.clf()\nax = sns.regplot(y=\"Age\", x=\"BMI\", data=nhanes_train, lowess=True, scatter_kws={'alpha':0.1}, line_kws={'color':\"r\"})\nax.set_xlim([10, 50])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3.2.4 Number of predictors is less than the number of samples\nSometimes, in machine learning, we have more predictors than the number of samples. This is called a high dimensional problem. Our regression method will not work here and we need to find ways to reduce the number of predictors.\nWe recommend evaluate the assumptions of your linear regression model on the training set before evaluating it on the test set.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#extesnsions-of-the-linear-model",
    "href": "02-Regression.html#extesnsions-of-the-linear-model",
    "title": "3  Regression",
    "section": "3.3 Extesnsions of the Linear Model",
    "text": "3.3 Extesnsions of the Linear Model\n\n3.3.1 Polynomial Regression\nLet’s go back to revisit the non-linearity of responder-predictor relationship. To deal with the slight curve in the residual plot, we can extend our model to accommodate non-linear relationships via Polynomial Regression.\nHere is what polynomial regression is capable, visually:\n\n\n\nSource: https://madrury.github.io/jekyll/update/statistics/2017/08/04/basis-expansions.html\n\n\nRecall our original equation:\n\\[\nMeanBloodPressure= \\beta_0 + \\beta_1 \\cdot Age\n\\] We can include transformed versions of the predictors to have other shapes, such as a quadratic shape:\n\\[\nMeanBloodPressure= \\beta_0 + \\beta_1 \\cdot Age + \\beta_2 \\cdot Age^2\n\\]\nThis is still a linear model – we have added a new predictor that gives us a quadratic shape. We use the poly() function to generate our polynomial predictor.\n\ny_train, X_train = model_matrix(\"MeanBloodPressure ~ poly(Age, degree=2, raw=True)\", nhanes_train)\n\nlinear_reg = linear_model.LinearRegression()\nlinear_reg = linear_reg.fit(X_train, y_train)\ny_train_predicted = linear_reg.predict(X_train)\n\nplt.clf()\nplt.scatter(X_train[X_train.columns[1]], y_train, alpha=.2)\nplt.scatter(X_train[X_train.columns[1]], y_train_predicted, color=\"red\")\nplt.xlabel('Age')\nplt.ylabel('Mean Blood Pressure')\nplt.show()\n\n\n\n\n\n\n\n\nLet’s look at our Residual Plot:\n\nresidual = y_train - y_train_predicted\n\nplt.clf()\nplt.scatter(y_train_predicted, residual, alpha=.2)\nplt.xlabel('Predicted response')\nplt.ylabel('Residual')\nplt.show()\n\n\n\n\n\n\n\n\nOkay, looks better! Less of a curved trend in the residual plot.\nIn general, it is rather unusual to see the polynomial term grow beyond 4, as they become more overly flexible and take on more strange shapes. We will take a look at the consequence of this in a moment.\n\n\n3.3.2 Interactions\nSuppose we think that \\(BMI\\) and \\(Gender\\) may be good predictors of \\(MeanBloodPressure\\):\nLet’s explore the relationship between \\(MeanBloodPressure\\) and \\(BMI\\) separately for values of \\(Gender\\).\n\nplt.clf()\nax = sns.lmplot(y=\"MeanBloodPressure\", x=\"BMI\", hue=\"Gender\", data=nhanes_train, lowess=False, scatter_kws={'alpha':0.1})\nax.set(xlim=(10, 50)) \nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nThe only model we know that relates all of these variables is:\n\\[\nMeanBloodPressure= \\beta_0 + \\beta_1 \\cdot BMI + \\beta_2 \\cdot Gender\n\\]\nAccording to our model, the relationship between \\(BMI\\) and \\(MeanBloodPressure\\) is a linear line with slope \\(\\beta_1\\), and the additional predictor of \\(Gender\\) will change our prediction by only a constant, \\(\\beta_2\\). Visually, that would look like two parallel lines, with \\(\\beta_2\\) dictating the distance between parallel lines. However, this plot suggests that our original model isn’t quite right: the additional predictor of \\(Gender\\) changes our prediction by more than a constant - it is dependent on \\(BMI\\) also.\nWhen multiple predictors have an synergistic effect on the outcome, their effect on the outcome occurs jointly - this is called an Interaction. To incorporate this into our model, we add an interaction term:\n\\[\nMeanBloodPressure= \\beta_0 + \\beta_1 \\cdot BMI + \\beta_2 \\cdot Gender + \\beta_3 \\cdot BMI \\cdot Gender\n\\]\nLet’s see what happens:\n\ny_train, X_train = model_matrix(\"MeanBloodPressure ~ BMI + Gender + BMI*Gender\", nhanes_train)\nlinear_reg = linear_model.LinearRegression()\nlinear_reg = linear_reg.fit(X_train, y_train)\ny_train_predicted = linear_reg.predict(X_train)\n\nplt.clf()\nplt.scatter(X_train.BMI, y_train, alpha=.2)\nplt.scatter(X_train.BMI, y_train_predicted, color=\"red\")\nplt.xlabel('BMI')\nplt.ylabel('Mean Blood Pressure')\nplt.show()\n\n\n\n\n\n\n\n\nWhich creates unique, non-parallel lines depending on the value of \\(Gender\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#overfitting",
    "href": "02-Regression.html#overfitting",
    "title": "3  Regression",
    "section": "3.4 Overfitting",
    "text": "3.4 Overfitting\nLast week, we discussed as the model learns from any data, it will learn to recognize its patterns, and sometimes it will recognize patterns that are only specific to this data and not reproducible anywhere else. This is called Overfitting, and why we constructed the training and testing datasets to identify this phenomena.\nLet’s look about overfitting in more detail: there can be different magnitudes of overfitting. The more flexible models we employ, the higher risk there will be overfitting, because these models will identify patterns too specific to the training data and not generalize to the test data. For instance, linear regression is a fairly inflexible approach, because it just uses a straight line to model the data. However, if we use polynomial regression, especially for higher degree polynomials, the model becomes more flexible, with a higher risk of overfitting.\nLet’s take a look at our first model again:\n\\[\nMeanBloodPressure= \\beta_0 + \\beta_1 \\cdot Age\n\\]\n\n#Use a small part of the data to illlustrate overfitting.\nnhanes_tiny = nhanes.sample(n=300, random_state=2)\n\ny, X = model_matrix(\"MeanBloodPressure ~ BMI\", nhanes_tiny)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\nlinear_model = sm.OLS(y_train, X_train).fit()\n\nplt.clf()\nfig, (ax1, ax2) = plt.subplots(2, layout='constrained')\n\nax1.plot(X_train.BMI, linear_model.predict(X_train), label=\"fitted line\")\nax1.scatter(X_train.BMI, y_train, alpha=.5, color=\"brown\", label=\"Training set\")\nax1.set(xlabel='BMI', ylabel='Mean Blood Pressure')\nax1.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\nax1.set_ylim(np.min(nhanes_tiny.MeanBloodPressure), np.max(nhanes_tiny.MeanBloodPressure))\ntrain_err = np.mean((linear_model.predict(X_train) - y_train.MeanBloodPressure) ** 2)\nax1.set_title('Training Error: ' + str(round(train_err, 2)))\n\nax2.plot(X_test.BMI, linear_model.predict(X_test), label=\"fitted line\")\nax2.scatter(X_test.BMI, y_test, alpha=.5, color=\"brown\", label=\"Testing set\")\nax2.set(xlabel='BMI', ylabel='Mean Blood Pressure')\nax2.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\nax2.set_ylim(np.min(nhanes_tiny.MeanBloodPressure), np.max(nhanes_tiny.MeanBloodPressure))\ntest_err = np.mean((linear_model.predict(X_test) - y_test.MeanBloodPressure) ** 2)\nax2.set_title('Testing Error: ' + str(round(test_err, 2)))\n\nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe see that Training Error &lt; Testing Error.\nLet’s look at what happens if we increase the flexibility of the model by fitting it with degree 2 polynomial:\n\np_degree = 2\ny, X = model_matrix(\"MeanBloodPressure ~ BMI + poly(BMI, degree=\" + str(p_degree) + \")\", nhanes_tiny)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nX_train_BMI = X_train.BMI\nX_test_BMI = X_test.BMI\nX_train.drop('BMI', axis=1, inplace=True)\nX_test.drop('BMI', axis=1, inplace=True)\n\nmodel = sm.OLS(y_train, X_train)\nlinear_model = model.fit()\n\nplt.clf()\nfig, (ax1, ax2) = plt.subplots(2, layout='constrained')\n\nax1.scatter(X_train_BMI, y_train, alpha=.5, color=\"brown\", label=\"Training set\")\nax1.scatter(X_train_BMI, linear_model.predict(), label=\"fitted line\")\n\nax1.set(xlabel='BMI', ylabel='Mean Blood Pressure')\ntrain_err = np.mean((linear_model.predict(X_train) - y_train.MeanBloodPressure) ** 2)\nax1.set_title('Training Error: ' + str(round(train_err, 2)))\nax1.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\nax1.set_ylim(np.min(nhanes_tiny.MeanBloodPressure), np.max(nhanes_tiny.MeanBloodPressure))\n\nax2.scatter(X_test_BMI, y_test, alpha=.5, color=\"brown\", label=\"Testing set\")\nax2.scatter(X_test_BMI, linear_model.predict(X_test), label=\"fitted line\")\nax2.set(xlabel='BMI', ylabel='Blood Pressure')\ntest_err = np.mean((linear_model.predict(X_test) - y_test.MeanBloodPressure) ** 2)\nax2.set_title('Testing Error: ' + str(round(test_err, 2)))\nax2.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\nax2.set_ylim(np.min(nhanes_tiny.MeanBloodPressure), np.max(nhanes_tiny.MeanBloodPressure))\n\nfig.suptitle('Polynomial Degree: ' + str(p_degree))\nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe see that both Training and Testing error both decreased slightly!\nWhat happens if we keep increasing the model complexity?\n\nfor p_degree in [4, 10]:\n  y, X = model_matrix(\"MeanBloodPressure ~ BMI + poly(BMI, degree=\" + str(p_degree) + \")\", nhanes_tiny)\n  \n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n  \n  X_train_BMI = X_train.BMI\n  X_test_BMI = X_test.BMI\n  X_train.drop('BMI', axis=1, inplace=True)\n  X_test.drop('BMI', axis=1, inplace=True)\n  \n  model = sm.OLS(y_train, X_train)\n  linear_model = model.fit()\n  \n  plt.clf()\n  fig, (ax1, ax2) = plt.subplots(2, layout='constrained')\n  \n  ax1.scatter(X_train_BMI, y_train, alpha=.5, color=\"brown\", label=\"Training set\")\n  ax1.scatter(X_train_BMI, linear_model.predict(), label=\"fitted line\")\n  \n  ax1.set(xlabel='BMI', ylabel='Mean Blood Pressure')\n  train_err = np.mean((linear_model.predict(X_train) - y_train.MeanBloodPressure) ** 2)\n  ax1.set_title('Training Error: ' + str(round(train_err, 2)))\n  ax1.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\n  ax1.set_ylim(np.min(nhanes_tiny.MeanBloodPressure), np.max(nhanes_tiny.MeanBloodPressure))\n\n  ax2.scatter(X_test_BMI, y_test, alpha=.5, color=\"brown\", label=\"Testing set\")\n  ax2.scatter(X_test_BMI, linear_model.predict(X_test), label=\"fitted line\")\n  ax2.set(xlabel='BMI', ylabel='Mean Blood Pressure')\n  test_err = np.mean((linear_model.predict(X_test) - y_test.MeanBloodPressure) ** 2)\n  ax2.set_title('Testing Error: ' + str(round(test_err, 2)))\n  ax2.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\n  ax2.set_ylim(np.min(nhanes_tiny.MeanBloodPressure), np.max(nhanes_tiny.MeanBloodPressure))\n\n  fig.suptitle('Polynomial Degree: ' + str(p_degree))\n  plt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nLet’s summarize it:\n\ntrain_err = []\ntest_err = []\npolynomials = list(range(1, 10))\n\nfor p_degree in polynomials:\n  if p_degree == 1:\n    y, X = model_matrix(\"MeanBloodPressure ~ BMI\", nhanes_tiny)\n  else:\n    y, X = model_matrix(\"MeanBloodPressure ~ poly(BMI, degree=\" + str(p_degree) + \")\", nhanes_tiny)\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n  \n  model = sm.OLS(y_train, X_train)\n  linear_model = model.fit()\n\n  train_err.append(np.mean((linear_model.predict(X_train) - y_train.MeanBloodPressure) ** 2))\n  test_err.append(np.mean((linear_model.predict(X_test) - y_test.MeanBloodPressure) ** 2))\n  \nplt.clf()\nplt.plot(polynomials, train_err, color=\"blue\", label=\"Training Error\")\nplt.plot(polynomials, test_err, color=\"red\", label=\"Testing Error\")\nplt.xlabel('Polynomial Degree')\nplt.ylabel('Error')\nplt.legend()\nplt.show()  \n\n\n\n\n\n\n\n\nAs our Polynomial Degree increased, the following happened:\n\nIn the linear model, we see that the Training Error is fairly high, and the Testing Error is even higher. This makes sense, as the model does not generalize as well to the testing set.\nAs the degrees increased, both training and testing error decreased slightly.\nAfter degree 4, we see that the Training Error continued to decrease, but the Testing Error blew up! This is an example of Overfitting, in which our model fitted the shape of of the training set so well that it fails to generalize to the testing set at all.\n\nWe want to find a model that is “just right” that doesn’t underfit or overfit the data. Usually, as the model becomes more flexible, the Training Error keeps lowering, and the Testing Error will lower a bit before increasing. It seems that our ideal prediction model is around a polynomial of degree 4, with the minimal Testing Error.\n\n3.4.1 Another example\nHere is another illustration of the phenomena, using synthetic controlled data:\nOn the left shows the Training Data in black dots. Then, three models are displayed: linear regression (orange line), two other models of increasing complexity in blue and green.\nOn the right shows the training error (grey curve), testing error (red curve), and where each of the three models on the left land in the error rate with their respective colors.\n\n\n\nSource: An Introduction to Statistical Learning, Ch. 2, by Gareth James, Daniela Witten, Trevor Hastie, Roebert Tibshirani, Jonathan Taylor.\n\n\nLastly, we recommend examining interactive tutorial: https://mlu-explain.github.io/bias-variance/\nHopefully you start to see the importance of examining the Testing Error instead of the Training Error to evaluate our model. A highly flexible data will overfit the model and make it seem like the Training Error is small, but it will not generalize to the Testing data, which will have Testing Error.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#appendix-inference",
    "href": "02-Regression.html#appendix-inference",
    "title": "3  Regression",
    "section": "3.5 Appendix: Inference",
    "text": "3.5 Appendix: Inference\nFor this course, we focus on prediction from our machine learning models. These models have an equally important usage in statistical inference. This appendix gives a quick overview of what that is about.\n\n3.5.1 Population and Sample\nThe way we formulate machine learning model is based on some fundamental concepts in inferential statistics. We will refresh this quickly in the context of our problem. Recall the following definitions:\nPopulation: The entire collection of individual units that a researcher is interested to study. For NHANES, this could be the entire US population.\nSample: A smaller collection of individual units that the researcher has selected to study. For NHANES, this could be a random sampling of the US population.\nIf the concepts of population, sample, estimation, p-value, and confidence interval is new to you, we recommend do a bit of reading here https://www.nature.com/collections/qghhqm/pointsofsignificance.\n\n\n3.5.2 Parameter inference\nNow let’s examine the function \\(f()\\)’s trained values, which are called parameters.\nFor the Linear Model, the model we first fitted was of the following form:\n\\[\nBloodPressure=\\beta_0 + \\beta_1 \\cdot BMI\n\\]\nwhich is an equation of a line.\n\\(\\beta_0\\) is a parameter describing the intercept of the line, and \\(\\beta_1\\) is a parameter describing the slope of the line.\nSuppose that from fitting the model on the Training Set, \\(\\beta_1=2\\). That means increasing \\(BMI\\) by 1 will lead to an increase of \\(BloodPressure\\) by 2. This measures the strength of association between a variable and the outcome.\nLet’s see this in practice:\n\nimport statsmodels.api as sm\n\ny, X = model_matrix(\"MeanBloodPressure ~ BMI\", nhanes_tiny)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\nlinear_model = sm.OLS(y_train, X_train).fit()\n\nlinear_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nMeanBloodPressure\nR-squared:\n0.078\n\n\nModel:\nOLS\nAdj. R-squared:\n0.071\n\n\nMethod:\nLeast Squares\nF-statistic:\n10.70\n\n\nDate:\nWed, 18 Feb 2026\nProb (F-statistic):\n0.00138\n\n\nTime:\n22:38:51\nLog-Likelihood:\n-502.67\n\n\nNo. Observations:\n128\nAIC:\n1009.\n\n\nDf Residuals:\n126\nBIC:\n1015.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n69.8978\n4.044\n17.284\n0.000\n61.895\n77.901\n\n\nBMI\n0.4600\n0.141\n3.271\n0.001\n0.182\n0.738\n\n\n\n\n\n\n\n\nOmnibus:\n2.841\nDurbin-Watson:\n2.019\n\n\nProb(Omnibus):\n0.242\nJarque-Bera (JB):\n2.458\n\n\nSkew:\n-0.190\nProb(JB):\n0.293\n\n\nKurtosis:\n3.562\nCond. No.\n106.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nBased on the output, \\(\\beta_0=69\\), \\(\\beta_1=.55\\). We also see associated standard errors, p-values, and confidence intervals. This is necessarily to report and interpret because we derive these parameters based on a sample of the data (train or test set), so there are statistical uncertainties associated with them. For instance, the 95% confidence interval of true population parameter will fall between (.22, .87). Notice that we used a different package called statsmodels to look the model inference.\n\n3.5.2.1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#section-1",
    "href": "02-Regression.html#section-1",
    "title": "3  Regression",
    "section": "3.6 ",
    "text": "3.6",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(3), 379–423. https://ieeexplore.ieee.org/document/6773024\nTuring, A. M. (1950). Computing Machinery and Intelligence. Mind, 59(236), 433–460. https://mind.oxfordjournals.org/content/LIX/236/433\nTuring, A. M. (1936). On Computable Numbers, with an Application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, s2-42(1), 230–265. https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf\nThompson, K. (1984). Reflections on Trusting Trust. Communications of the ACM, 27(8), 761–763. https://dl.acm.org/doi/10.1145/358198.358210\nGhemawat, S., Gobioff, H., & Leung, S.-T. (2003). The Google File System. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles (pp. 29–43). https://research.google.com/archive/gfs-sosp2003.pdf",
    "crumbs": [
      "References"
    ]
  }
]