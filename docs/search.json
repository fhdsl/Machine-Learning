[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "1 Machine Learning",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Machine Learning",
    "section": "1.1 Target Audience",
    "text": "1.1 Target Audience\nThe course is intended for folks with basic Python programming experience who are interested in predictive modeling and model interpretation from a machine learning perspective. The course is also appropriate for scientists and clinicians who are interested to communicate with data scientists to understand the ins and outs of a machine learning problem. The pre-requisites for the course is Intro to Python, or being able to use Lists and Pandas Dataframes to manipulate data. Basic knowledge of statistics, such as hypothesis testing and p-values, is also strongly recommended.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#curriculum",
    "href": "index.html#curriculum",
    "title": "Machine Learning",
    "section": "1.2 Curriculum",
    "text": "1.2 Curriculum\nThe course covers the framework of machine learning for predictive modeling and model interpretation from a practitioner’s perspective. You will be able to implement several popular machine learning techniques based on the question of interest and the dataset at hand. You will then evaluate the model based on their performance and diagnostics to understand its strengths and limitations. Technical mathematics and algorithms will not be emphasized.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Machine Learning",
    "section": "1.3 Learning Objectives",
    "text": "1.3 Learning Objectives\n\nCompare machine learning models in terms of flexibility vs. Interpretability.\nCompare machine learning model performance in terms of overfitting and underfitting.\nImplement and Interpret models such as linear regression, logistic regression, and lasso using a Tidy dataset via existing packages such as Sklearn and Statsmodels.\nEvaluate model performance metrics for inference and prediction, such as AIC, BIC, MSE, and AUC, under a cross validated framework if appropriate.\nExplain the difference in machine learning techniques between low and high dimensional data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html",
    "href": "01-Problem-Setup.html",
    "title": "2  Problem Set-Up",
    "section": "",
    "text": "2.1 Classification model example\nSuppose that we are given the National Health And Nutrition Examination Survey (NHANES) dataset and want to build a machine learning model to classify whether a person has hypertension blood pressure based on clinical and demographic variables.\nUsing algebraic expressions, we formulate the following:\n\\[\nHypertension=f(Age, BMI)\n\\]\nWhere \\(f(Age, BMI)\\) is a machine learning model that takes in the variables \\(Age\\), \\(BMI\\), and make a classification on whether someone has \\(Hypertension\\).\nA machine learning model, such as the one described above, has two main uses:\nLet’s start with the easiest case for just \\(Hypertension = f(Age)\\), a single predictor.\nBefore we fit models, we often visualize the data to get a sense whether our setup makes sense.\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom formulaic import model_matrix\nimport statsmodels.api as sm\n\nnhanes = pd.read_csv(\"classroom_data/NHANES.csv\")\nnhanes['Hypertension'] = (nhanes['BPDiaAve'] &gt; 80) | (nhanes['BPSysAve'] &gt; 130)\n\nnhanes['Hypertension2'] = nhanes['Hypertension'].replace({True: \"Hypertension\", False: \"No Hypertension\"})\nplt.clf()\nax = sns.boxplot(y=\"Hypertension2\", x=\"BMI\", data=nhanes)\nax.set_ylabel('')\nplt.show()\nOkay, great, it looks like when someone’s BMI is higher, then it is more likely that the person has Hypertension.\nNow, let’s build the model \\(Hypertension = f(BMI)\\) to make a prediction of \\(Hyptertension\\) given \\(BMI\\).\ny, X = model_matrix(\"Hypertension ~ BMI\", nhanes)\nlogit_model = sm.Logit(y, X).fit() \n\nplt.clf()\nplt.scatter(X.BMI, logit_model.predict(), color=\"blue\", label=\"Fitted Line\")\nplt.scatter(X.BMI, y, alpha=.3, color=\"brown\", label=\"Data\")\nplt.xlabel('BMI')\nplt.ylabel('Probability of Hypertension')\nplt.legend()\nplt.show()\n\nOptimization terminated successfully.\n         Current function value: 0.515543\n         Iterations 6\nInstead of boxplots, we plotted the data just using points, with “Hypertension” having a probability of 1 and “No Hypertension” having a probability of 0. We see that we have a fitted line in blue for every value of BMI, which represents our machine learning model \\(f(BMI)\\). This model is called Logistic Regression.\nThe first thing we want to investigate about this model is how well it performs in terms of Classification. Just using \\(BMI\\) as a variable, what is the Accuracy of \\(f(BMI)\\) classifying whether a person has \\(Hypertension\\)? Notice that first \\(f(BMI)\\) gives us continuous probability values, such as given a BMI of 30, there is a 20% chance the person has Hypertension. We need a discrete cutoff of this model to decide whether the person has Hypertension.\nA reasonable cutoff to start is 50%: if the probability of having Hypertension is &gt;=50%, then classify that person having Hypertension. Same for &lt; 50%. This is called the Decision Boundary.\nplt.clf()\nplt.scatter(X.BMI, logit_model.predict(), color=\"blue\", label=\"Fitted Line\")\nplt.scatter(X.BMI, y, alpha=.3, color=\"brown\", label=\"Data\")\nplt.xlabel('BMI')\nplt.ylabel('Probability of Hypertension')\nplt.axhline(y=0.5, color='r', linestyle='--', label='Prediction Cutoff')\nplt.legend();\nplt.show()\nGiven this decision boundary, what is the accuracy?\nfrom sklearn.metrics import (confusion_matrix, accuracy_score)\n\nprediction_cut = [1 if x &gt;= .5 else 0 for x in logit_model.predict()]\nprint('Accuracy = ', accuracy_score(y, prediction_cut)) \n\nAccuracy =  0.762300186838281\nOkay, that’s a starting point!\nWe can break down classification accuracy to four additional results:\ntn, fp, fn, tp = confusion_matrix(y, prediction_cut).ravel().tolist()\nprint(\"True Positive:\", tp, \"\\nFalse Positive: \", fp, \"\\nTrue Negative: \", tn, \"\\nFalse Negative:\", fn)\n\nTrue Positive: 109 \nFalse Positive:  137 \nTrue Negative:  7235 \nFalse Negative: 2153\ndefine tp, fp, tn, fn\ndefine confusion matrix\ncm = confusion_matrix(y, prediction_cut) \nprint(\"Confusion Matrix : \\n\", cm) \n\nConfusion Matrix : \n [[7235  137]\n [2153  109]]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#classification-model-example",
    "href": "01-Problem-Setup.html#classification-model-example",
    "title": "2  Problem Set-Up",
    "section": "",
    "text": "Classification and Prediction (Focus of this course): How accurately can we classify or predict the outcome?\n\nClassification: Given a new person’s \\(Age, BMI\\), classify whether the person has \\(Hyptertension\\). The outcome is a yes/no classification.\nPrediction: Given a person’s \\(Age, BMI\\), predict the person’s \\(BloodPressure\\) value. The outcome is a continuous value.\n\nInference (Secondary in this course): Which predictors are associated with the response, and how strong is the association?\n\nClassification model example: What is the odds ratio of of \\(Age\\) on \\(Hyptertension\\)? If the odds ratio of \\(Age\\) on \\(Hyptertension\\) is 2, then an increase of 1 in \\(Age\\) increases the odds of \\(Hyptertension\\) by 2.\nPrediction model example: Suppose the model is described as \\(BloodPressure = f(Age,BMI)=20 + 3 \\cdot Age - .2 \\cdot BMI\\). Each variable has a relationship to the outcome: an increase of \\(Age\\) by 1 will lead to an increase of \\(BloodPressure\\) by 3. This measures the strength of association between a variable and the outcome.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1.1 Summary of Example\nSo what have we done so far? / Preview of what is to come:\n\nSelected a predictor and binary outcome, and visualized it\n\nEventually we will look at a continuous outcome, multiple predictors, and how to select multiple predictors\n\nFit it to a logistic regression model, which is a classification model\n\nLogistic regression is a type of linear model, which is the basis for most machine learning models\n\nWe evaluated the model in terms of accuracy, true positive rate, true negative rate, false positive rate, false negative rate\n\nWe evaluated the model on the same data that we built the model. Ideally, we want to evaluate the model on data it has never seen before. More on this next week.\n\n\nBefore we race ahead….there’s a lot of new Python data structures that we are working with in this course. So let’s brush up on our data structures and how to make sense of the new ones coming our way!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#review-of-data-structures",
    "href": "01-Problem-Setup.html#review-of-data-structures",
    "title": "2  Problem Set-Up",
    "section": "2.2 Review of Data Structures",
    "text": "2.2 Review of Data Structures\nWe will be seeing a lot of different data structures in this course beyond DataFrames, Series, and Lists. So let’s review how we think about learning new data structures to make our lives easier when we encounter new data structures.\nLet’s review the List data structure. For any data structure, we ask the following:\n\nWhat does it contain (in terms of data)?\nWhat can it do (in terms of functions)?\n\nAnd if it “makes sense” to us, then it is well-designed data structure.\nFormally, a data structure in Python (also known as an Object) may contain the following:\n\nValue that holds the essential data for the data structure.\nAttributes that hold subset or additional data for the data structure.\nFunctions called Methods that are for the data structure and have to take in the variable referenced as an input.\n\nLet’s see how this applies to the List:\n\nValue: the contents of the list, such as [2, 3, 4].\nAttributes that store additional values: Not relevant for lists.\nMethods that can be used on the object: my_list.append(x)\n\nHow about Dataframe?\n\nValue: the 2-dimensional spreadsheet of the dataframe.\nAttributes that store additional values: df.shape gives the number of rows and columns. df.my_col_name access the column called “my_col_name”.\nMethods that can be used on the object: df.merge(other_df, on=“column_name”)\n\nFeel free to look at the cheatsheet on data structures from Intro to Python to refresh yourself.\n\n2.2.1 NumPy\nA new Data Structure we will work with in this course is NumPy’s ndarray (“n-dimensional array”) data structure. It is commonly referred as “NumPy Array”. It is very similar to a Dataframe, but has the following characteristics for building machine learning models:\n\nAll elements are homogeneous and numeric.\nThere are no column or row names.\nMathematical operations are optimized to be fast.\n\nSo, let’s see some examples:\n\nValue: the 2-dimensional numerical table. It actually can be any dimension, but we will just work with 1-dimensional (similar to a List) and 2-dimensional.\nAttributes that store additional values:\n\nTwo-dimensional subsetting, similar to lists: data[:5, :3] subsets for for the first 5 rows and first three columns. data[:5, [0, 2, 3]] subsets for the first 5 rows and 1st, 3rd, and 4th columns.\ndata.shape gives the shape of the NumPy Array. data.dim will tell you the number of dimensions of the NumPy Array.\n\nMethods that can be used on the object:\n\ndata.sum(axis=0) sums over rows, data.sum(axis=1) sums over columns.\n\n\nFor this course, we often load in a dataset in the Pandas Dataframe format, and then once we pick the our outcome and predictors, we will transform the Dataframe into an NumPy Array, such as this line of code we saw earlier: y, X = model_matrix(\"Hypertension ~ BMI\", nhanes). We specify our outcome, predictor, and Dataframe for the model_matrix() function, and the outputs are two NumPy Arrays, one for the outcome, and one for the predictors. Any downstream Machine Learning modeling work off the NumPy Arrays y and X.\nMore introduction can be found on NumPy’s tutorial guide.\n\n\n2.2.2 What is this data structure?\nIf you are not sure about what your variable’s data structure, use the type() function, such as type(mystery_data) and it will tell you.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#exercises",
    "href": "01-Problem-Setup.html#exercises",
    "title": "2  Problem Set-Up",
    "section": "2.3 Exercises",
    "text": "2.3 Exercises\nExercises for week 1 can be found here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "02-Regression.html",
    "href": "02-Regression.html",
    "title": "3  Regression",
    "section": "",
    "text": "3.1 Population and Sample\nThe way we formulate machine learning model is based on some fundamental concepts in inferential statistics. We will refresh this quickly in the context of our problem. Recall the following definitions:\nPopulation: The entire collection of individual units that a researcher is interested to study. For NHANES, this could be the entire US population.\nSample: A smaller collection of individual units that the researcher has selected to study. For NHANES, this could be a random sampling of the US population.\nIn Machine Learning problems, we often like to take two, non-overlapping samples from the population: the Training Set, and the Test Set. We train our model using the Training Set, which gives us a function \\(f()\\) that relates the predictors to the outcome. Then, for 2 main use cases:\nIf the concepts of population, sample, estimation, p-value, and confidence interval is new to you, we recommend do a bit of reading here https://www.nature.com/collections/qghhqm/pointsofsignificance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#population-and-sample",
    "href": "02-Regression.html#population-and-sample",
    "title": "3  Regression",
    "section": "",
    "text": "Classification and Prediction: We use the trained model to classify or predict the outcome using predictors from the Test Set and compare to the true value in the Test Set.\nInference: We examine the function \\(f()\\)’s trained values, which are called parameters. Because these parameters are derived from the Training Set, they are an estimated quantity from a sample, similar to other summary statistics like the mean of a sample. Therefore, to say anything about the true population, we have to use statistical tools such as p-values and confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#how-to-evaluate-and-pick-a-model",
    "href": "02-Regression.html#how-to-evaluate-and-pick-a-model",
    "title": "3  Regression",
    "section": "3.3 How to evaluate and pick a model?",
    "text": "3.3 How to evaluate and pick a model?\nThe little example model we showcased above is an example of a linear model, but we will look at several types of models in this course. In order to decide how to evaluate and pick a model, we will need to develop a framework to assess a model. Let’s start with the use case of prediction.\n\n3.3.1 Prediction\nSuppose we try to use the single variable \\(BMI\\) to predict \\(BloodPressure\\) using a linear model.\nWe examine how well our model performs in terms of prediction by seeing how close our model’s predicted \\(BloodPressure\\) is to the Training Set’s true \\(BloodPressure\\): the Training Error. We also take the model to the Testing Set to predict \\(BloodPressure\\) using predictors from the Test Set and compare to the true \\(BloodPressure\\) in the Test Set: the Testing Error. We want the model’s Training Error to be adequately small on the Training Set, but what we really care about is the Testing Error, because it is a true test of how the model performs on unseen, new data, and allows us to see how generalizeable the model is.\nOkay, let’s how it does on the Training Set:\n\n#np.mean((results.fittedvalues - y_train.BloodPressure) ** 2)\n\n\n#results.mse_resid\n\n[graph here]\nAnd then on the Test Set:\n\n#np.mean((results.predict(X_test) - y_test.BloodPressure) ** 2)\n\n\n#plt.plot(X_test.BMI, results.get_prediction(X_test).predicted_mean, label=\"fitted line\")\n#plt.scatter(X_test.BMI, y_test, alpha=.3, color=\"black\", label=\"test set\")\n#plt.legend();\n\nWe see that the Training Error is fairly high, and the Testing Error is even higher. This is an example of Underfitting, where our model failed to capture the complexity of the data in both the Training and Testing Set.\nLet’s return to the drawing board and fit a new type of model that has more flexibility around complicated patterns of data. Let’s see how it does on the Training Set:\n\n#y, X = model_matrix(\"BloodPressure ~ poly(BMI, degree=5)\", nhanes)\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n#model = sm.OLS(y_train, X_train)\n#results = model.fit()\n#results.summary()\n\n#plt.plot(X_train.BMI, results.fittedvalues, label=\"fitted line\")\n#plt.scatter(X_train.BMI, y_train, alpha=.3, color=\"brown\", label=\"training set\")\n#plt.legend();\n\n[graph here]\nAnd then on the Test Set:\n[graph here]\nWe see that the Training Error is low, but the Testing Error is huge! This is an example of Overfitting, in which our model fitted the shape of of the training set so well that it fails to generalize to the testing set.\nWe want to find a model that is “just right” that doesn’t underfit or overfit the data. Usually, as the model becomes more flexible, the Training Error keeps lowering, and the Testing Error will lower a bit before increasing. See below:\n\n\n\nSource: An Introduction to Statistical Learning, Ch. 2, by Gareth James, Daniela Witten, Trevor Hastie, Roebert Tibshirani, Jonathan Taylor.\n\n\nAlso see this interactive tutorial: https://mlu-explain.github.io/bias-variance/\n\n\n3.3.2 Inference\nLet’s consider how we would evaluate and choose models for Inference.\nFor models with low number of predictors, there are some plots and metrics one would consider, such as BIC.\nFor models with high number of predictors, we will talk about it in more detail in weeks 5 & 6.\nBesides how flexible a model is, another categorization of machine models is how interpretable they are. The more interpretable a model is, the better one can describe how each variable has an predictor of the model. That makes the inference process easier.\nBelow are some example models mapped to these two dichotomies. The linear model lies very similar as the “Least Squares” models.\n\n\n\nSource: An Introduction to Statistical Learning, Ch. 2, by Gareth James, Daniela Witten, Trevor Hastie, Roebert Tibshirani, Jonathan Taylor",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#the-numpy-package",
    "href": "02-Regression.html#the-numpy-package",
    "title": "3  Regression",
    "section": "3.4 The NumPy Package",
    "text": "3.4 The NumPy Package\n\n3.4.1 Subsetting\n\n\n3.4.2 How to split the data for training and testing",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#linear-regression-preview",
    "href": "02-Regression.html#linear-regression-preview",
    "title": "3  Regression",
    "section": "3.5 Linear Regression Preview?",
    "text": "3.5 Linear Regression Preview?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "02-Regression.html#appendix-other-terms",
    "href": "02-Regression.html#appendix-other-terms",
    "title": "3  Regression",
    "section": "3.6 Appendix: Other terms",
    "text": "3.6 Appendix: Other terms\nParametric vs. Non-parametric\nBias-Variance trade-off\nSupervised vs. Unsupervised",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(3), 379–423. https://ieeexplore.ieee.org/document/6773024\nTuring, A. M. (1950). Computing Machinery and Intelligence. Mind, 59(236), 433–460. https://mind.oxfordjournals.org/content/LIX/236/433\nTuring, A. M. (1936). On Computable Numbers, with an Application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, s2-42(1), 230–265. https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf\nThompson, K. (1984). Reflections on Trusting Trust. Communications of the ACM, 27(8), 761–763. https://dl.acm.org/doi/10.1145/358198.358210\nGhemawat, S., Gobioff, H., & Leung, S.-T. (2003). The Google File System. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles (pp. 29–43). https://research.google.com/archive/gfs-sosp2003.pdf",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "02-Regression.html#generalizability-of-a-model",
    "href": "02-Regression.html#generalizability-of-a-model",
    "title": "3  Regression",
    "section": "3.2 Generalizability of a model",
    "text": "3.2 Generalizability of a model\nThe above section connects our Training and Testing sets to the statistical concepts of Sample and Population. But why do we need Training and Testing sets in Classification and Prediction? Moreover, why is the Training and Testing accuracy different in the first week’s exercise?\n\nThere is always going to be some variation, because Training and Test are samples from the population, as we discussed above.\nThe model was trained on the Training Set, so the prediction is going to naturally perform better than data it has never seen before on the Testing Set.\n\nHow do we interpret the difference in error between Training and Testing Set?\nIs it always the case that the Testing Error is greater than the Training Error?\n\n\nTo see how generalizable a model is, let’s look at a problem in prediction, not classification. The underlying principles of model generalizability applies to both prediction and classification, but it is generally easier to visualize this concept in prediction models.\nIn a prediction model, the outcome is continuous. Instead of yes/no for \\(Hyptertension\\), we will use the clinical metric to \\(BloodPressure\\), which is a formula based on ____. We use \\(Age\\) as our predictor, which was something we explored in the first exercise. The prediction model we will use is called a Linear Model, in its basic form draws a straight line as a line of “best fit” for given the data.\nTo evaluate the model, we will let the model make prediction given the input variable. This prediction is on the scale of \\(BloodPressure\\), so there’s no to draw a Decision Boundary in prediction problems. To examine the accuracy, we will look at the mean squared error between the true \\(BloodPressure\\) and the predicted \\(BloodPressure\\). Let’s take a look.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom formulaic import model_matrix\nimport statsmodels.api as sm\n\nnhanes = pd.read_csv(\"classroom_data/NHANES.csv\")\nnhanes['BloodPressure'] = nhanes['BPDiaAve'] + (nhanes['BPSysAve'] - nhanes['BPDiaAve']) / 3 \n\nnhanes_tiny = nhanes.sample(n=300, random_state=1)\n\ny, X = model_matrix(\"BloodPressure ~ BMI\", nhanes_tiny)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\nmodel = sm.OLS(y_train, X_train)\nlinear_model = model.fit()\n\nplt.clf()\nfig, (ax1, ax2) = plt.subplots(2, layout='constrained')\n\nax1.plot(X_train.BMI, linear_model.predict(X_train), label=\"fitted line\")\nax1.scatter(X_train.BMI, y_train, alpha=.5, color=\"brown\", label=\"Training set\")\nax1.set(xlabel='BMI', ylabel='Blood Pressure')\nax1.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\nax1.set_ylim(np.min(nhanes_tiny.BloodPressure), np.max(nhanes_tiny.BloodPressure))\ntrain_err = np.mean((linear_model.predict(X_train) - y_train.BloodPressure) ** 2)\nax1.set_title('Training Error: ' + str(round(train_err, 2)))\n\nax2.plot(X_test.BMI, linear_model.predict(X_test), label=\"fitted line\")\nax2.scatter(X_test.BMI, y_test, alpha=.5, color=\"brown\", label=\"Testing set\")\nax2.set(xlabel='BMI', ylabel='Blood Pressure')\nax2.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\nax2.set_ylim(np.min(nhanes_tiny.BloodPressure), np.max(nhanes_tiny.BloodPressure))\ntest_err = np.mean((linear_model.predict(X_test) - y_test.BloodPressure) ** 2)\nax2.set_title('Testing Error: ' + str(round(test_err, 2)))\n\nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe see that Training Error &lt; Testing Error.\nIt turns out that the relationship between Training Error and Testing Error is connected to the Model Complexity.\nA model is more complex relative to another model if:\n\nIt contains more predictors\nThe relationship between a predictor and response is fit via a higher order polynomial or smooth function\n\nLet’s look at what happens if we increase the complexity of the model by fitting it with a more smooth function. We use a polynomial function of order 2.\n\np_degree = 2\ny, X = model_matrix(\"BloodPressure ~ BMI + poly(BMI, degree=\" + str(p_degree) + \")\", nhanes_tiny)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\nX_train_BMI = X_train.BMI\nX_test_BMI = X_test.BMI\nX_train.drop('BMI', axis=1, inplace=True)\nX_test.drop('BMI', axis=1, inplace=True)\n\nmodel = sm.OLS(y_train, X_train)\nlinear_model = model.fit()\n\nplt.clf()\nfig, (ax1, ax2) = plt.subplots(2, layout='constrained')\n\nax1.scatter(X_train_BMI, y_train, alpha=.5, color=\"brown\", label=\"Training set\")\nax1.scatter(X_train_BMI, linear_model.predict(), label=\"fitted line\")\n\nax1.set(xlabel='BMI', ylabel='Blood Pressure')\ntrain_err = np.mean((linear_model.predict(X_train) - y_train.BloodPressure) ** 2)\nax1.set_title('Training Error: ' + str(round(train_err, 2)))\nax1.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\nax1.set_ylim(np.min(nhanes_tiny.BloodPressure), np.max(nhanes_tiny.BloodPressure))\n\nax2.scatter(X_test_BMI, y_test, alpha=.5, color=\"brown\", label=\"Testing set\")\nax2.scatter(X_test_BMI, linear_model.predict(X_test), label=\"fitted line\")\nax2.set(xlabel='BMI', ylabel='Blood Pressure')\ntest_err = np.mean((linear_model.predict(X_test) - y_test.BloodPressure) ** 2)\nax2.set_title('Testing Error: ' + str(round(test_err, 2)))\nax2.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\nax2.set_ylim(np.min(nhanes_tiny.BloodPressure), np.max(nhanes_tiny.BloodPressure))\n\nfig.suptitle('Polynomial Degree: ' + str(p_degree))\nplt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nWe see that both Training and Testing error both decreased noticeably! The Testing Error is still a little bit lower than the Training Error.\nWhat happens if we keep increasing the model complexity?\n\nfor p_degree in [4, 6, 10]:\n  y, X = model_matrix(\"BloodPressure ~ BMI + poly(BMI, degree=\" + str(p_degree) + \")\", nhanes_tiny)\n  \n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n  \n  X_train_BMI = X_train.BMI\n  X_test_BMI = X_test.BMI\n  X_train.drop('BMI', axis=1, inplace=True)\n  X_test.drop('BMI', axis=1, inplace=True)\n  \n  model = sm.OLS(y_train, X_train)\n  linear_model = model.fit()\n  \n  plt.clf()\n  fig, (ax1, ax2) = plt.subplots(2, layout='constrained')\n  \n  ax1.scatter(X_train_BMI, y_train, alpha=.5, color=\"brown\", label=\"Training set\")\n  ax1.scatter(X_train_BMI, linear_model.predict(), label=\"fitted line\")\n  \n  ax1.set(xlabel='BMI', ylabel='Blood Pressure')\n  train_err = np.mean((linear_model.predict(X_train) - y_train.BloodPressure) ** 2)\n  ax1.set_title('Training Error: ' + str(round(train_err, 2)))\n  ax1.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\n  ax1.set_ylim(np.min(nhanes_tiny.BloodPressure), np.max(nhanes_tiny.BloodPressure))\n\n  ax2.scatter(X_test_BMI, y_test, alpha=.5, color=\"brown\", label=\"Testing set\")\n  ax2.scatter(X_test_BMI, linear_model.predict(X_test), label=\"fitted line\")\n  ax2.set(xlabel='BMI', ylabel='Blood Pressure')\n  test_err = np.mean((linear_model.predict(X_test) - y_test.BloodPressure) ** 2)\n  ax2.set_title('Testing Error: ' + str(round(test_err, 2)))\n  ax2.set_xlim(np.min(nhanes_tiny.BMI), np.max(nhanes_tiny.BMI))\n  ax2.set_ylim(np.min(nhanes_tiny.BloodPressure), np.max(nhanes_tiny.BloodPressure))\n\n  fig.suptitle('Polynomial Degree: ' + str(p_degree))\n  plt.show()\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 672x480 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nLet’s summarize it:\n\ntrain_err = []\ntest_err = []\npolynomials = list(range(1, 10))\n\nfor p_degree in polynomials:\n  if p_degree == 1:\n    y, X = model_matrix(\"BloodPressure ~ BMI\", nhanes_tiny)\n  else:\n    y, X = model_matrix(\"BloodPressure ~ poly(BMI, degree=\" + str(p_degree) + \")\", nhanes_tiny)\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n  \n  model = sm.OLS(y_train, X_train)\n  linear_model = model.fit()\n\n  train_err.append(linear_model.mse_resid)\n  test_err.append(np.mean((linear_model.predict(X_test) - y_test.BloodPressure) ** 2))\n  \n  \n  \nplt.clf()\nplt.plot(polynomials, train_err, color=\"blue\", label=\"Training Error\")\nplt.plot(polynomials, test_err, color=\"red\", label=\"Testing Error\")\nplt.xlabel('Polynomial Degree')\nplt.ylabel('Error')\nplt.legend()\nplt.show()  \n\n\n\n\n\n\n\n\nAs our Polynomial Degree increased, the following happened:\n\nIn the linear model, we see that the Training Error is fairly high, and the Testing Error is even higher. This is an example of Underfitting, where our model failed to capture the complexity of the data in both the Training and Testing Set.\nAfter degree 4, we see that the Training Error is low, but the Testing Error is huge! This is an example of Overfitting, in which our model fitted the shape of of the training set so well that it fails to generalize to the testing set.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression</span>"
    ]
  }
]