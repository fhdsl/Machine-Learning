[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "1 Machine Learning",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Machine Learning",
    "section": "1.1 Target Audience",
    "text": "1.1 Target Audience\nThe course is intended for folks with basic Python programming experience who are interested in predictive modeling and model interpretation from a machine learning perspective. The course is also appropriate for scientists and clinicians who are interested to communicate with data scientists to understand the ins and outs of a machine learning problem. The pre-requisites for the course is Intro to Python, or being able to use Lists and Pandas Dataframes to manipulate data. Basic knowledge of statistics, such as hypothesis testing and p-values, is also strongly recommended.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#curriculum",
    "href": "index.html#curriculum",
    "title": "Machine Learning",
    "section": "1.2 Curriculum",
    "text": "1.2 Curriculum\nThe course covers the framework of machine learning for predictive modeling and model interpretation from a practitioner’s perspective. You will be able to implement several popular machine learning techniques based on the question of interest and the dataset at hand. You will then evaluate the model based on their performance and diagnostics to understand its strengths and limitations. Technical mathematics and algorithms will not be emphasized.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "Machine Learning",
    "section": "1.3 Learning Objectives",
    "text": "1.3 Learning Objectives\n\nCompare machine learning models in terms of flexibility vs. Interpretability.\nCompare machine learning model performance in terms of overfitting and underfitting.\nImplement and Interpret models such as linear regression, logistic regression, and lasso using a Tidy dataset via existing packages such as Sklearn and Statsmodels.\nEvaluate model performance metrics for inference and prediction, such as AIC, BIC, MSE, and AUC, under a cross validated framework if appropriate.\nExplain the difference in machine learning techniques between low and high dimensional data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Machine Learning</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html",
    "href": "01-Problem-Setup.html",
    "title": "2  Problem Set-Up",
    "section": "",
    "text": "2.1 Population and Sample\nThe way we formulate machine learning model is based on some fundamental concepts in inferential statistics. We will refresh this quickly in the context of our problem. Recall the following definitions:\nPopulation: The entire collection of individual units that a researcher is interested to study. For NHANES, this could be the entire US population.\nSample: A smaller collection of individual units that the researcher has selected to study. For NHANES, this could be a random sampling of the US population.\nIn Machine Learning problems, we often like to take two, non-overlapping samples from the population: the Training Set, and the Test Set. We train our model using the Training Set, which gives us a function \\(f()\\) that relates the predictors to the outcome. Then, for our main use cases:\nIf the concepts of population, sample, estimation, p-value, and confidence interval is new to you, we recommend do a bit of reading here [todo].",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#population-and-sample",
    "href": "01-Problem-Setup.html#population-and-sample",
    "title": "2  Problem Set-Up",
    "section": "",
    "text": "Prediction: We use the trained model to predict the outcome using predictors from the Test Set and compare to the true value in the Test Set.\nInference: We examine the function \\(f()\\)’s trained values, which are called parameters. For instance, \\(f(Age,BMI,Income)=20 + 3 \\cdot Age - .2 \\cdot BMI + .00015 \\cdot Income\\), the values \\(20\\), \\(3\\), \\(-.2\\), and \\(.00015\\) are the parameters. Because these parameters are derived from the Training Set, they are an estimated quantity from a sample, similar to other summary statistics like the mean of a sample. Therefore, to say anything about the true population, we have to use statistical tools such as p-values and confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#how-to-evaluate-and-pick-a-model",
    "href": "01-Problem-Setup.html#how-to-evaluate-and-pick-a-model",
    "title": "2  Problem Set-Up",
    "section": "2.2 How to evaluate and pick a model?",
    "text": "2.2 How to evaluate and pick a model?\nThe little example model we showcased above is an example of a linear model, but we will look at several types of models in this course. In order to decide how to evaluate and pick a model, we will need to develop a framework to assess a model. Let’s start with the use case of prediction.\n\n2.2.1 Prediction\nSuppose we try to use the single variable \\(BMI\\) to predict \\(BloodPressure\\) using a linear model.\n\nimport pandas as pd\nimport seaborn as sns\nnhanes = pd.read_csv(\"classroom_data/NHANES.csv\")\nnhanes['BloodPressure'] = nhanes['BPDiaAve'] + (nhanes['BPSysAve'] - nhanes['BPDiaAve']) / 3 \n\nplot = sns.lmplot(x=\"BMI\", y=\"BloodPressure\", data=nhanes)\n\n\n\n\n\n\n\n\nWe examine how well our model performs in terms of prediction by seeing how close our model’s predicted \\(BloodPressure\\) is to the Training Set’s true \\(BloodPressure\\): the Training Error. We also take the model to the Testing Set to predict \\(BloodPressure\\) using predictors from the Test Set and compare to the true \\(BloodPressure\\) in the Test Set: the Testing Error. We want the model’s Training Error to be adequately small on the Training Set, but what we really care about is the Testing Error, because it is a true test of how the model performs on unseen, new data, and allows us to see how generalizeable the model is.\nOkay, let’s how it does on the Training Set:\n[graph here]\nAnd then on the Test Set:\n[graph here]\nWe see that the Training Error is fairly high, and the Testing Error is even higher. This is an example of Underfitting, where our model failed to capture the complexity of the data in both the Training and Testing Set.\nLet’s return to the drawing board and fit a new type of model that has more flexibility around complicated patterns of data. Let’s see how it does on the Training Set:\n[graph here]\nAnd then on the Test Set:\n[graph here]\nWe see that the Training Error is low, but the Testing Error is huge! This is an example of Overfitting, in which our model fitted the shape of of the training set so well that it fails to generalize to the testing set.\nWe want to find a model that is “just right” that doesn’t underfit or overfit the data. Usually, as the model becomes more flexible, the Training Error keeps lowering, and the Testing Error will lower a bit before increasing. See below:\n\n\n\nSource: An Introduction to Statistical Learning, Ch. 2, by Gareth James, Daniela Witten, Trevor Hastie, Roebert Tibshirani, Jonathan Taylor.\n\n\nAlso see this interactive tutorial: https://mlu-explain.github.io/bias-variance/\n\n\n2.2.2 Inference\nLet’s consider how we would evaluate and choose models for Inference.\nFor models with low number of predictors, there are some plots and metrics one would consider, such as BIC.\nFor models with high number of predictors, we will talk about it in more detail in weeks 5 & 6.\nBesides how flexible a model is, another categorization of machine models is how interpretable they are. The more interpretable a model is, the better one can describe how each variable has an predictor of the model. That makes the inference process easier.\nBelow are some example models mapped to these two dichotomies. The linear model lies very similar as the “Least Squares” models.\n\n\n\nSource: An Introduction to Statistical Learning, Ch. 2, by Gareth James, Daniela Witten, Trevor Hastie, Roebert Tibshirani, Jonathan Taylor",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#the-numpy-package",
    "href": "01-Problem-Setup.html#the-numpy-package",
    "title": "2  Problem Set-Up",
    "section": "2.3 The NumPy Package",
    "text": "2.3 The NumPy Package\n\n2.3.1 Subsetting\n\n\n2.3.2 How to split the data for training and testing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#linear-regression-preview",
    "href": "01-Problem-Setup.html#linear-regression-preview",
    "title": "2  Problem Set-Up",
    "section": "2.4 Linear Regression Preview?",
    "text": "2.4 Linear Regression Preview?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "01-Problem-Setup.html#appendix-other-terms",
    "href": "01-Problem-Setup.html#appendix-other-terms",
    "title": "2  Problem Set-Up",
    "section": "2.5 Appendix: Other terms",
    "text": "2.5 Appendix: Other terms\nParametric vs. Non-parametric\nBias-Variance trade-off\nSupervised vs. Unsupervised",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Problem Set-Up</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(3), 379–423. https://ieeexplore.ieee.org/document/6773024\nTuring, A. M. (1950). Computing Machinery and Intelligence. Mind, 59(236), 433–460. https://mind.oxfordjournals.org/content/LIX/236/433\nTuring, A. M. (1936). On Computable Numbers, with an Application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, s2-42(1), 230–265. https://www.cs.virginia.edu/~robins/Turing_Paper_1936.pdf\nThompson, K. (1984). Reflections on Trusting Trust. Communications of the ACM, 27(8), 761–763. https://dl.acm.org/doi/10.1145/358198.358210\nGhemawat, S., Gobioff, H., & Leung, S.-T. (2003). The Google File System. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles (pp. 29–43). https://research.google.com/archive/gfs-sosp2003.pdf",
    "crumbs": [
      "References"
    ]
  }
]